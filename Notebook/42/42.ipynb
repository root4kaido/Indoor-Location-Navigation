{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM baseline\n",
    "\n",
    "from kuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import scipy.interpolate\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from indoor_location_competition_20.io_f import read_data_file\n",
    "import indoor_location_competition_20.compute_f as compute_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/home/knikaido/work/Indoor-Location-Navigation/data/\")\n",
    "WIFI_DIR = DATA_DIR / 'indoorunifiedwifids_original'\n",
    "MLFLOW_DIR = DATA_DIR / 'mlflow/mlruns'\n",
    "OUTPUT_DIR = Path('./output/')\n",
    "MLFLOW_DIR = DATA_DIR / 'mlflow/mlruns'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'loss':{\n",
    "        'name': 'MSELoss',\n",
    "        'params':{}\n",
    "    },\n",
    "    'optimizer':{\n",
    "        'name': 'Adam',\n",
    "        'params':{\n",
    "            'lr': 0.01,\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'scheduler':{\n",
    "        'name': 'ReduceLROnPlateau',\n",
    "        'params':{\n",
    "            'factor': 0.1,\n",
    "            'patience': 3,\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'loader':{\n",
    "        'train':{\n",
    "            'batch_size': 512,\n",
    "            'shuffle': True,\n",
    "            'num_workers': 4,\n",
    "        },\n",
    "        'valid':{\n",
    "            'batch_size': 512,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 4,\n",
    "        },\n",
    "        'test':{\n",
    "            'batch_size': 512,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 4,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = configs\n",
    "\n",
    "# globals variable\n",
    "SEED = 777\n",
    "MAX_EPOCHS = 500\n",
    "N_SPLITS = 5\n",
    "DEBUG = False\n",
    "# EXP_MESSAGE = config['globals']['exp_message']\n",
    "\n",
    "EXP_NAME = 42\n",
    "IS_SAVE = True\n",
    "\n",
    "utils.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: wandb: not found\n"
     ]
    }
   ],
   "source": [
    "!wandb login e8aaf98060af90035c3c28a83b34452780aeec20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(WIFI_DIR / 'train_10_th10000_withcount.csv')\n",
    "test_df = pd.read_csv(WIFI_DIR / 'test_10_th10000_withcount.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(DATA_DIR/'indoor-location-navigation/sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BSSIDとRSSIは100ずつ存在しているけど全てが必要なわけではないみたい  \n",
    "ここでは20だけ取り出している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training target features\n",
    "NUM_FEATS = 80\n",
    "BSSID_FEATS = [f'bssid_{i}' for i in range(NUM_FEATS)]\n",
    "RSSI_FEATS  = [f'rssi_{i}' for i in range(NUM_FEATS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bssid_NはN個目のBSSIDを示しておりRSSI値が大きい順に番号が振られている。\n",
    "100個しかない\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSSID TYPES(train): 60711\n",
      "BSSID TYPES(test): 30185\n",
      "BSSID TYPES(all): 90896\n"
     ]
    }
   ],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "# train\n",
    "wifi_bssids = []\n",
    "# bssidを列ごとにリストに入れていく\n",
    "for i in BSSID_FEATS:\n",
    "    wifi_bssids.extend(train_df.loc[:,i].values.tolist())\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "\n",
    "train_wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(train): {train_wifi_bssids_size}')\n",
    "\n",
    "# test\n",
    "wifi_bssids_test = []\n",
    "for i in BSSID_FEATS:\n",
    "    wifi_bssids_test.extend(test_df.loc[:,i].values.tolist())\n",
    "wifi_bssids_test = list(set(wifi_bssids_test))\n",
    "\n",
    "test_wifi_bssids_size = len(wifi_bssids_test)\n",
    "print(f'BSSID TYPES(test): {test_wifi_bssids_size}')\n",
    "\n",
    "\n",
    "wifi_bssids.extend(wifi_bssids_test)\n",
    "# wifi_bssids = list(set(wifi_bssids))\n",
    "wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(all): {wifi_bssids_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSSI TYPES(train): 93\n",
      "RSSI TYPES(test): 77\n",
      "RSSI TYPES(all): 170\n"
     ]
    }
   ],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "# train\n",
    "rssi_bssids = []\n",
    "# bssidを列ごとにリストに入れていく\n",
    "for i in RSSI_FEATS:\n",
    "    rssi_bssids.extend(train_df.loc[:,i].values.tolist())\n",
    "rssi_bssids = list(set(rssi_bssids))\n",
    "\n",
    "train_rssi_bssids_size = len(rssi_bssids)\n",
    "print(f'RSSI TYPES(train): {train_rssi_bssids_size}')\n",
    "\n",
    "# test\n",
    "rssi_bssids_test = []\n",
    "for i in RSSI_FEATS:\n",
    "    rssi_bssids_test.extend(test_df.loc[:,i].values.tolist())\n",
    "rssi_bssids_test = list(set(rssi_bssids_test))\n",
    "\n",
    "test_rssi_bssids_size = len(rssi_bssids_test)\n",
    "print(f'RSSI TYPES(test): {test_rssi_bssids_size}')\n",
    "\n",
    "\n",
    "rssi_bssids.extend(rssi_bssids_test)\n",
    "# rssi_bssids = list(set(rssi_bssids))\n",
    "rssi_bssids_size = len(rssi_bssids)\n",
    "print(f'RSSI TYPES(all): {rssi_bssids_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ssid_0</th>\n",
       "      <th>ssid_1</th>\n",
       "      <th>ssid_2</th>\n",
       "      <th>ssid_3</th>\n",
       "      <th>ssid_4</th>\n",
       "      <th>ssid_5</th>\n",
       "      <th>ssid_6</th>\n",
       "      <th>ssid_7</th>\n",
       "      <th>ssid_8</th>\n",
       "      <th>ssid_9</th>\n",
       "      <th>...</th>\n",
       "      <th>frequency_98</th>\n",
       "      <th>frequency_99</th>\n",
       "      <th>raw_count</th>\n",
       "      <th>wp_tmestamp</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>floor</th>\n",
       "      <th>floor_str</th>\n",
       "      <th>path_id</th>\n",
       "      <th>site_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140</td>\n",
       "      <td>cef6dc5e595dd99c3b2c605de65cfc1f147e892b</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>8c1562bec17e1425615f3402f72dded3caa42ce5</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>...</td>\n",
       "      <td>5745</td>\n",
       "      <td>2452</td>\n",
       "      <td>100</td>\n",
       "      <td>1578469851129</td>\n",
       "      <td>157.99141</td>\n",
       "      <td>102.125390</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>5e158ef61506f2000638fd1f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>cef6dc5e595dd99c3b2c605de65cfc1f147e892b</td>\n",
       "      <td>ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>...</td>\n",
       "      <td>5180</td>\n",
       "      <td>5320</td>\n",
       "      <td>100</td>\n",
       "      <td>1578469857653</td>\n",
       "      <td>162.93443</td>\n",
       "      <td>106.413020</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>5e158ef61506f2000638fd1f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cef6dc5e595dd99c3b2c605de65cfc1f147e892b</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140</td>\n",
       "      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n",
       "      <td>...</td>\n",
       "      <td>5745</td>\n",
       "      <td>5745</td>\n",
       "      <td>100</td>\n",
       "      <td>1578469857653</td>\n",
       "      <td>162.93443</td>\n",
       "      <td>106.413020</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>5e158ef61506f2000638fd1f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cef6dc5e595dd99c3b2c605de65cfc1f147e892b</td>\n",
       "      <td>ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>...</td>\n",
       "      <td>5745</td>\n",
       "      <td>5180</td>\n",
       "      <td>100</td>\n",
       "      <td>1578469857653</td>\n",
       "      <td>162.93443</td>\n",
       "      <td>106.413020</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>5e158ef61506f2000638fd1f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n",
       "      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>5731b8e08abc69d4c4d685c58164059207c93310</td>\n",
       "      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n",
       "      <td>ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140</td>\n",
       "      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n",
       "      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n",
       "      <td>...</td>\n",
       "      <td>2452</td>\n",
       "      <td>5765</td>\n",
       "      <td>100</td>\n",
       "      <td>1578469862177</td>\n",
       "      <td>168.49713</td>\n",
       "      <td>109.861336</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>5e158ef61506f2000638fd1f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251108</th>\n",
       "      <td>ea7731d04cf9ed352d4805b1ff904bebdf60eb49</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>b5d43f6b4e1938ed497c7b589c6eae9ac0bee168</td>\n",
       "      <td>0a8a55cf161bc4980194ec9f7f7a448439be4b74</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>1f09251bbfadafb11c63c87963af25238d6bc886</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1573733061352</td>\n",
       "      <td>203.53165</td>\n",
       "      <td>143.513960</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F7</td>\n",
       "      <td>5dcd5c9323759900063d590a</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251109</th>\n",
       "      <td>ea7731d04cf9ed352d4805b1ff904bebdf60eb49</td>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>b5d43f6b4e1938ed497c7b589c6eae9ac0bee168</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1573733070079</td>\n",
       "      <td>192.57130</td>\n",
       "      <td>145.781450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F7</td>\n",
       "      <td>5dcd5c9323759900063d590a</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251110</th>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>ea7731d04cf9ed352d4805b1ff904bebdf60eb49</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>b5d43f6b4e1938ed497c7b589c6eae9ac0bee168</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>5d998a8668536c4f51004c25f474117fe9555f78</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1573733070079</td>\n",
       "      <td>192.57130</td>\n",
       "      <td>145.781450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F7</td>\n",
       "      <td>5dcd5c9323759900063d590a</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251111</th>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ea7731d04cf9ed352d4805b1ff904bebdf60eb49</td>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>b5d43f6b4e1938ed497c7b589c6eae9ac0bee168</td>\n",
       "      <td>0a8a55cf161bc4980194ec9f7f7a448439be4b74</td>\n",
       "      <td>5d998a8668536c4f51004c25f474117fe9555f78</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1573733070079</td>\n",
       "      <td>192.57130</td>\n",
       "      <td>145.781450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F7</td>\n",
       "      <td>5dcd5c9323759900063d590a</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251112</th>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d</td>\n",
       "      <td>ea7731d04cf9ed352d4805b1ff904bebdf60eb49</td>\n",
       "      <td>4abd3985ba804364272767c04cdc211615f77c56</td>\n",
       "      <td>b5d43f6b4e1938ed497c7b589c6eae9ac0bee168</td>\n",
       "      <td>5d998a8668536c4f51004c25f474117fe9555f78</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>0a8a55cf161bc4980194ec9f7f7a448439be4b74</td>\n",
       "      <td>5eddd2b748b50cbfa67d90f0ef019e6e822e38f5</td>\n",
       "      <td>1556355684145fce5e67ba749d943a180266ad90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1573733070079</td>\n",
       "      <td>192.57130</td>\n",
       "      <td>145.781450</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F7</td>\n",
       "      <td>5dcd5c9323759900063d590a</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251113 rows × 408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ssid_0  \\\n",
       "0       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "1       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "2       cef6dc5e595dd99c3b2c605de65cfc1f147e892b   \n",
       "3       cef6dc5e595dd99c3b2c605de65cfc1f147e892b   \n",
       "4       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "...                                          ...   \n",
       "251108  ea7731d04cf9ed352d4805b1ff904bebdf60eb49   \n",
       "251109  ea7731d04cf9ed352d4805b1ff904bebdf60eb49   \n",
       "251110  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251111  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251112  1556355684145fce5e67ba749d943a180266ad90   \n",
       "\n",
       "                                          ssid_1  \\\n",
       "0       ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140   \n",
       "1       cef6dc5e595dd99c3b2c605de65cfc1f147e892b   \n",
       "2       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "3       ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140   \n",
       "4       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "...                                          ...   \n",
       "251108  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251109  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251110  ea7731d04cf9ed352d4805b1ff904bebdf60eb49   \n",
       "251111  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251112  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "\n",
       "                                          ssid_2  \\\n",
       "0       cef6dc5e595dd99c3b2c605de65cfc1f147e892b   \n",
       "1       ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140   \n",
       "2       ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140   \n",
       "3       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "4       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "...                                          ...   \n",
       "251108  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251109  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251110  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251111  ea7731d04cf9ed352d4805b1ff904bebdf60eb49   \n",
       "251112  ea7731d04cf9ed352d4805b1ff904bebdf60eb49   \n",
       "\n",
       "                                          ssid_3  \\\n",
       "0       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "1       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "2       b9f0208be00bd8b337be7f12e02e3a3ce846e22b   \n",
       "3       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "4       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "...                                          ...   \n",
       "251108  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251109  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251110  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251111  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251112  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "\n",
       "                                          ssid_4  \\\n",
       "0       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "1       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "2       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "3       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "4       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "...                                          ...   \n",
       "251108  b5d43f6b4e1938ed497c7b589c6eae9ac0bee168   \n",
       "251109  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251110  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251111  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251112  b5d43f6b4e1938ed497c7b589c6eae9ac0bee168   \n",
       "\n",
       "                                          ssid_5  \\\n",
       "0       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "1       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "2       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "3       b9f0208be00bd8b337be7f12e02e3a3ce846e22b   \n",
       "4       5731b8e08abc69d4c4d685c58164059207c93310   \n",
       "...                                          ...   \n",
       "251108  0a8a55cf161bc4980194ec9f7f7a448439be4b74   \n",
       "251109  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251110  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251111  b5d43f6b4e1938ed497c7b589c6eae9ac0bee168   \n",
       "251112  5d998a8668536c4f51004c25f474117fe9555f78   \n",
       "\n",
       "                                          ssid_6  \\\n",
       "0       8c1562bec17e1425615f3402f72dded3caa42ce5   \n",
       "1       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "2       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "3       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "4       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "...                                          ...   \n",
       "251108  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251109  b5d43f6b4e1938ed497c7b589c6eae9ac0bee168   \n",
       "251110  b5d43f6b4e1938ed497c7b589c6eae9ac0bee168   \n",
       "251111  0a8a55cf161bc4980194ec9f7f7a448439be4b74   \n",
       "251112  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "\n",
       "                                          ssid_7  \\\n",
       "0       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "1       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "2       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "3       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "4       ea4a14e0d5bcdd20703fbe3bbc90f70b171ff140   \n",
       "...                                          ...   \n",
       "251108  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "251109  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251110  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251111  5d998a8668536c4f51004c25f474117fe9555f78   \n",
       "251112  0a8a55cf161bc4980194ec9f7f7a448439be4b74   \n",
       "\n",
       "                                          ssid_8  \\\n",
       "0       b9f0208be00bd8b337be7f12e02e3a3ce846e22b   \n",
       "1       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "2       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "3       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "4       b9f0208be00bd8b337be7f12e02e3a3ce846e22b   \n",
       "...                                          ...   \n",
       "251108  1f09251bbfadafb11c63c87963af25238d6bc886   \n",
       "251109  4abd3985ba804364272767c04cdc211615f77c56   \n",
       "251110  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d   \n",
       "251111  1556355684145fce5e67ba749d943a180266ad90   \n",
       "251112  5eddd2b748b50cbfa67d90f0ef019e6e822e38f5   \n",
       "\n",
       "                                          ssid_9  ... frequency_98  \\\n",
       "0       b7e6027447eb1f81327d66cfd3adbe557aabf26c  ...         5745   \n",
       "1       b7e6027447eb1f81327d66cfd3adbe557aabf26c  ...         5180   \n",
       "2       b9f0208be00bd8b337be7f12e02e3a3ce846e22b  ...         5745   \n",
       "3       d839a45ebe64ab48b60a407d837fb01d3c0dfef9  ...         5745   \n",
       "4       7182afc4e5c212133d5d7d76eb3df6c24618302b  ...         2452   \n",
       "...                                          ...  ...          ...   \n",
       "251108  1556355684145fce5e67ba749d943a180266ad90  ...            0   \n",
       "251109  1556355684145fce5e67ba749d943a180266ad90  ...            0   \n",
       "251110  5d998a8668536c4f51004c25f474117fe9555f78  ...            0   \n",
       "251111  ad82e27aa3cd9f276fd3a5146fa8c7c5e5b5207d  ...            0   \n",
       "251112  1556355684145fce5e67ba749d943a180266ad90  ...            0   \n",
       "\n",
       "       frequency_99 raw_count    wp_tmestamp          x           y floor  \\\n",
       "0              2452       100  1578469851129  157.99141  102.125390  -1.0   \n",
       "1              5320       100  1578469857653  162.93443  106.413020  -1.0   \n",
       "2              5745       100  1578469857653  162.93443  106.413020  -1.0   \n",
       "3              5180       100  1578469857653  162.93443  106.413020  -1.0   \n",
       "4              5765       100  1578469862177  168.49713  109.861336  -1.0   \n",
       "...             ...       ...            ...        ...         ...   ...   \n",
       "251108            0       100  1573733061352  203.53165  143.513960   6.0   \n",
       "251109            0       100  1573733070079  192.57130  145.781450   6.0   \n",
       "251110            0       100  1573733070079  192.57130  145.781450   6.0   \n",
       "251111            0       100  1573733070079  192.57130  145.781450   6.0   \n",
       "251112            0       100  1573733070079  192.57130  145.781450   6.0   \n",
       "\n",
       "       floor_str                   path_id site_id  \n",
       "0             B1  5e158ef61506f2000638fd1f       0  \n",
       "1             B1  5e158ef61506f2000638fd1f       0  \n",
       "2             B1  5e158ef61506f2000638fd1f       0  \n",
       "3             B1  5e158ef61506f2000638fd1f       0  \n",
       "4             B1  5e158ef61506f2000638fd1f       0  \n",
       "...          ...                       ...     ...  \n",
       "251108        F7  5dcd5c9323759900063d590a      23  \n",
       "251109        F7  5dcd5c9323759900063d590a      23  \n",
       "251110        F7  5dcd5c9323759900063d590a      23  \n",
       "251111        F7  5dcd5c9323759900063d590a      23  \n",
       "251112        F7  5dcd5c9323759900063d590a      23  \n",
       "\n",
       "[251113 rows x 408 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(wifi_bssids)\n",
    "le_site = LabelEncoder()\n",
    "le_site.fit(train_df['site_id'])\n",
    "le_rssi = LabelEncoder()\n",
    "le_rssi.fit(rssi_bssids)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "\n",
    "def preprocess(input_df, le=le, le_site=le_site, ss=ss):\n",
    "    output_df = input_df.copy()\n",
    "    # RSSIの正規化\n",
    "#     output_df.loc[:,RSSI_FEATS] = ss.transform(input_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "    # BSSIDのLE(1からふる)\n",
    "    for i in BSSID_FEATS:\n",
    "        output_df.loc[:,i] = le.transform(input_df.loc[:,i])\n",
    "#         output_df.loc[:,i] = output_df.loc[:,i] + 1  # 0からではなく1から番号を振りたいため なぜ？\n",
    "    for i in RSSI_FEATS:\n",
    "        output_df.loc[:,i] = le_rssi.transform(input_df.loc[:,i])\n",
    "\n",
    "    # site_idのLE\n",
    "    output_df.loc[:, 'site_id'] = le_site.transform(input_df.loc[:, 'site_id'])\n",
    "\n",
    "    # なぜ２重でやる？\n",
    "#     output_df.loc[:,RSSI_FEATS] = ss.transform(output_df.loc[:,RSSI_FEATS])\n",
    "    return output_df\n",
    "\n",
    "train = preprocess(train_df)\n",
    "test = preprocess(test_df)\n",
    "\n",
    "train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_count = len(train['site_id'].unique())\n",
    "site_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch model\n",
    "- embedding layerが重要  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IndoorDataset(Dataset):\n",
    "    def __init__(self, df, phase='train'):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "        self.bssid_feats = df[BSSID_FEATS].values.astype(int)\n",
    "        self.rssi_feats = df[RSSI_FEATS].values.astype(int)\n",
    "        self.site_id = df['site_id'].values.astype(int)\n",
    "        self.raw_count = df['raw_count'].values.astype(int)\n",
    "\n",
    "        if phase in ['train', 'valid']:\n",
    "            self.xy = df[['x', 'y']].values.astype(np.float32)\n",
    "            self.floor = df['floor'].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bssid = self.bssid_feats[idx]\n",
    "        rssi = self.rssi_feats[idx]\n",
    "        \n",
    "        if self.phase != 'test'\n",
    "            if np.random.rand() < 0.3:\n",
    "                tyohuku = np.where(rssi == pd.Series(rssi[:min(self.raw_count[idx], NUM_FEATS)]).value_counts().index[0])[0]\n",
    "                indexes = np.arange(len(rssi))\n",
    "                indexes[tyohuku[0]:tyohuku[-1]+1] = np.random.permutation(tyohuku)\n",
    "                bssid = bssid[indexes]\n",
    "                rssi = rssi[indexes]\n",
    "        \n",
    "        concat_feat = np.empty(2 * NUM_FEATS).astype(int)\n",
    "        concat_feat[0::2] = bssid\n",
    "        concat_feat[1::2] = rssi\n",
    "        \n",
    "        feature = {\n",
    "            'RSSI_BSSID_FEATS':concat_feat,\n",
    "            'site_id':self.site_id[idx]\n",
    "        }\n",
    "        if self.phase in ['train', 'valid']:\n",
    "            target = {\n",
    "                'xy':self.xy[idx],\n",
    "                'floor':self.floor[idx]\n",
    "            }\n",
    "        else:\n",
    "            target = {}\n",
    "        return feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, bssid_size=94248, site_size=24, embedding_dim=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # bssid\n",
    "        # ->64次元に圧縮後sequence化にする\n",
    "        # wifi_bssids_sizeが辞書の数を表す\n",
    "        self.bssid_embedding = nn.Embedding(bssid_size, 64, max_norm=True)\n",
    "        # site\n",
    "        # ->2次元に圧縮後sequence化する\n",
    "        # site_countが辞書の数を表す       \n",
    "        self.site_embedding = nn.Embedding(site_size, 64, max_norm=True)\n",
    "\n",
    "        # rssi\n",
    "        # 次元を64倍に線形変換\n",
    "#         self.rssi_embedding = nn.Embedding(rssi_size, 64, max_norm=True)\n",
    "#         self.rssi = nn.Sequential(\n",
    "#             nn.BatchNorm1d(NUM_FEATS),\n",
    "#             nn.Linear(NUM_FEATS, NUM_FEATS * 64)\n",
    "#         )\n",
    "        \n",
    "        concat_size = 64 + (2 * NUM_FEATS * 64)\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(concat_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(concat_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(concat_size)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.linear1 = nn.Linear(in_features=concat_size, out_features=256)#, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1)\n",
    "        self.lstm1 = nn.LSTM(input_size=256,hidden_size=128,dropout=0.3, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128,hidden_size=16,dropout=0.1, batch_first=True)\n",
    "\n",
    "        self.fc_xy = nn.Linear(16, 2)\n",
    "        # self.fc_x = nn.Linear(16, 1)\n",
    "        # self.fc_y = nn.Linear(16, 1)\n",
    "        self.fc_floor = nn.Linear(16, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input embedding\n",
    "        batch_size = x[\"site_id\"].shape[0]\n",
    "        x_bssid = self.bssid_embedding(x['RSSI_BSSID_FEATS'])\n",
    "        x_bssid = self.flatten(x_bssid)\n",
    "        \n",
    "        x_site_id = self.site_embedding(x['site_id'])\n",
    "        x_site_id = self.flatten(x_site_id)\n",
    "\n",
    "#         x_rssi = self.rssi_embedding(x['RSSI_FEATS'])\n",
    "#         x_rssi = self.flatten(x_rssi)\n",
    "\n",
    "        x = torch.cat([x_bssid, x_site_id], dim=1)\n",
    "        x = self.linear_layer2(x)\n",
    "\n",
    "        # lstm layer\n",
    "        x = x.view(batch_size, 1, -1)  # [batch, 1]->[batch, 1, 1]\n",
    "        x = self.batch_norm1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = torch.relu(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # output [batch, 1, 1] -> [batch]\n",
    "        # x_ = self.fc_x(x).view(-1)\n",
    "        # y_ = self.fc_y(x).view(-1)\n",
    "        xy = self.fc_xy(x).squeeze(1)\n",
    "        floor = torch.relu(self.fc_floor(x)).view(-1)\n",
    "        # return {\"x\":x_, \"y\":y_, \"floor\":floor} \n",
    "        return {\"xy\": xy, \"floor\": floor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_position_error(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model: nn.Module, config: dict):\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer_name = optimizer_config.get(\"name\")\n",
    "    base_optimizer_name = optimizer_config.get(\"base_name\")\n",
    "    optimizer_params = optimizer_config['params']\n",
    "\n",
    "    if hasattr(optim, optimizer_name):\n",
    "        optimizer = optim.__getattribute__(optimizer_name)(model.parameters(), **optimizer_params)\n",
    "        return optimizer\n",
    "    else:\n",
    "        base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        optimizer = globals().get(optimizer_name)(\n",
    "            model.parameters(), \n",
    "            base_optimizer,\n",
    "            **optimizer_config[\"params\"])\n",
    "        return  optimizer\n",
    "\n",
    "def get_scheduler(optimizer, config: dict):\n",
    "    scheduler_config = config[\"scheduler\"]\n",
    "    scheduler_name = scheduler_config.get(\"name\")\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **scheduler_config[\"params\"])\n",
    "\n",
    "\n",
    "def get_criterion(config: dict):\n",
    "    loss_config = config[\"loss\"]\n",
    "    loss_name = loss_config[\"name\"]\n",
    "    loss_params = {} if loss_config.get(\"params\") is None else loss_config.get(\"params\")\n",
    "    if hasattr(nn, loss_name):\n",
    "        criterion = nn.__getattribute__(loss_name)(**loss_params)\n",
    "    else:\n",
    "        criterion = globals().get(loss_name)(**loss_params)\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.xy_criterion = get_criterion(config)\n",
    "        self.f_criterion = get_criterion(config)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        loss = self.xy_criterion(output[\"xy\"], y[\"xy\"])\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        xy_loss = self.xy_criterion(output[\"xy\"], y[\"xy\"])\n",
    "        f_loss = self.f_criterion(output[\"floor\"], y[\"floor\"])\n",
    "        loss = xy_loss  # + f_loss\n",
    "        mpe = mean_position_error(\n",
    "            to_np(output['xy'][:, 0]), to_np(output['xy'][:, 1]), 0, \n",
    "            to_np(y['xy'][:, 0]), to_np(y['xy'][:, 1]), 0)\n",
    "        \n",
    "        # floor lossは現状は無視して良い\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/xy', xy_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/floor', f_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'MPE/val', mpe, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return mpe\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = np.mean(outputs)\n",
    "        print(f'epoch = {self.current_epoch}, mpe_loss = {avg_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model, self.config)\n",
    "        scheduler = get_scheduler(optimizer, self.config)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof\n",
    "def evaluate(model, loaders, phase):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    f_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders[phase]:\n",
    "            x, y = batch\n",
    "            output = model(x)\n",
    "            x_list.append(to_np(output['xy'][:, 0]))\n",
    "            y_list.append(to_np(output['xy'][:, 1]))\n",
    "            f_list.append(to_np(output['floor']))\n",
    "\n",
    "    x_list = np.concatenate(x_list)\n",
    "    y_list = np.concatenate(y_list)\n",
    "    f_list = np.concatenate(f_list)\n",
    "    return x_list, y_list, f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 0\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/user/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msqrt4kaido\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.25<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">hopeful-surf-370</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sqrt4kaido/Indoor_Location_Navigation\" target=\"_blank\">https://wandb.ai/sqrt4kaido/Indoor_Location_Navigation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sqrt4kaido/Indoor_Location_Navigation/runs/1ahel2u9\" target=\"_blank\">https://wandb.ai/sqrt4kaido/Indoor_Location_Navigation/runs/1ahel2u9</a><br/>\n",
       "                Run data is saved locally in <code>/home/knikaido/work/Indoor-Location-Navigation/Git/Notebook/42/wandb/run-20210423_031012-1ahel2u9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type      | Params\n",
      "-------------------------------------------\n",
      "0 | model        | LSTMModel | 11.4 M\n",
      "1 | xy_criterion | MSELoss   | 0     \n",
      "2 | f_criterion  | MSELoss   | 0     \n",
      "-------------------------------------------\n",
      "11.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 M    Total params\n",
      "45.419    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, mpe_loss = 165.9820556640625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c3f7910704cc7b4bac9c678faa899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, mpe_loss = 131.36253927375537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, mpe_loss = 105.54240417193526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2, mpe_loss = 91.10954490569799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3, mpe_loss = 84.69453374221969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4, mpe_loss = 82.55621765353155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5, mpe_loss = 64.85292022806738\n"
     ]
    }
   ],
   "source": [
    "oofs = np.zeros((len(train), 2), dtype = np.float32)  # 全てのoofをdfで格納する\n",
    "predictions = []  # 全ての予測値をdfで格納する\n",
    "val_scores = []\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "# for fold, (trn_idx, val_idx) in enumerate(skf.split(train.loc[:, 'path'], train.loc[:, 'path'])):\n",
    "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train.loc[:, 'path_id'], groups=train.loc[:, 'path_id'])):\n",
    "\n",
    "    # 指定したfoldのみループを回す\n",
    "\n",
    "    print('=' * 20)\n",
    "    print(f'Fold {fold}')\n",
    "    print('=' * 20)\n",
    "\n",
    "    # train/valid data\n",
    "    trn_df = train.loc[trn_idx, BSSID_FEATS + RSSI_FEATS + ['raw_count', 'site_id', 'x','y','floor']].reset_index(drop=True)\n",
    "    val_df = train.loc[val_idx, BSSID_FEATS + RSSI_FEATS + ['raw_count', 'site_id', 'x','y','floor']].reset_index(drop=True)\n",
    "\n",
    "    # data loader\n",
    "    loaders = {}\n",
    "    loader_config = config[\"loader\"]\n",
    "    loaders[\"train\"] = DataLoader(IndoorDataset(trn_df, phase=\"train\"), **loader_config[\"train\"], worker_init_fn=worker_init_fn) \n",
    "    loaders[\"valid\"] = DataLoader(IndoorDataset(val_df, phase=\"valid\"), **loader_config[\"valid\"], worker_init_fn=worker_init_fn)\n",
    "    loaders[\"test\"] = DataLoader(IndoorDataset(test, phase=\"test\"), **loader_config[\"test\"], worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    # model\n",
    "    model = LSTMModel(wifi_bssids_size+rssi_bssids_size, site_count)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # loggers\n",
    "    RUN_NAME = f'exp{str(EXP_NAME)}'\n",
    "    wandb.init(project='Indoor_Location_Navigation', entity='sqrt4kaido', group=RUN_NAME, job_type=RUN_NAME + f'-fold-{fold}')\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{fold}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = model_name\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "\n",
    "    learner = Learner(model, config)\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=OUTPUT_DIR,\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{learner.current_epoch}-{fold}')\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='Loss/val',\n",
    "        min_delta=0.00,\n",
    "        patience=20,\n",
    "        verbose=True,\n",
    "        mode='min')\n",
    "    callbacks.append(early_stop_callback)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=loggers,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        default_root_dir=OUTPUT_DIR,\n",
    "        gpus=1,\n",
    "        fast_dev_run=DEBUG,\n",
    "        deterministic=True,\n",
    "        benchmark=True,\n",
    "#         precision=16,\n",
    "#         progress_bar_refresh_rate=0  # vscodeの時progress barの動作が遅いので表示しない\n",
    "        )\n",
    "\n",
    "\n",
    "    trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "\n",
    "    #############\n",
    "    # validation (to make oof)\n",
    "    #############\n",
    "    model.eval()\n",
    "    oof_x, oof_y, oof_f = evaluate(model, loaders, phase=\"valid\")\n",
    "    oofs[val_idx, 0] = oof_x\n",
    "    oofs[val_idx, 1] = oof_y\n",
    "\n",
    "    \n",
    "    val_score = mean_position_error(\n",
    "        oof_x, oof_y, 0,\n",
    "        val_df['x'].values, val_df['y'].values, 0)\n",
    "    val_scores.append(val_score)\n",
    "    print(f\"fold {fold}: mean position error {val_score}\")\n",
    "\n",
    "    #############\n",
    "    # inference\n",
    "    #############\n",
    "    preds_x, preds_y, preds_f = evaluate(model, loaders, phase=\"test\")\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = sub.columns\n",
    "    test_preds[\"site_path_timestamp\"] = test[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    predictions.append(test_preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df = pd.DataFrame(oofs, columns=['x', 'y'])\n",
    "oofs_df['path'] = train_df['path_id']\n",
    "oofs_df['timestamp'] = train_df['wp_tmestamp']\n",
    "oofs_df['site'] = train_df['site_id']\n",
    "oofs_df['site_path_timestamp'] = oofs_df['site'] + '_' + oofs_df['path'] + '_' + oofs_df['timestamp'].astype(str)\n",
    "oofs_df['floor'] = train_df['floor']\n",
    "oofs_df.to_csv(str(OUTPUT_DIR) + f\"/oof{EXP_NAME}.csv\", index=False)\n",
    "oofs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV:7.194082754397348\n"
     ]
    }
   ],
   "source": [
    "oofs_score = mean_position_error(\n",
    "        oofs_df['x'], oofs_df['y'], 0,\n",
    "        train_df['x'].values, train_df['y'].values, 0)\n",
    "print(f\"CV:{oofs_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = pd.concat(predictions).groupby('site_path_timestamp').mean()\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_37 = pd.read_csv('../37/output/sub37.csv', index_col=0)\n",
    "all_preds_37.index = pd.read_csv(WIFI_DIR / 'test_7_th20000.csv')['site_path_timestamp']\n",
    "all_preds_37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_merge = pd.merge(all_preds_37, all_preds, how='left', on='site_path_timestamp')[['floor_y', 'x_y', 'y_y']]\n",
    "all_preds_merge = all_preds_merge.rename(columns={'floor_y': 'floor', 'x_y': 'x', 'y_y': 'y'})\n",
    "all_preds_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_merge['floor'].fillna(all_preds_37['floor'], inplace=True)\n",
    "all_preds_merge['x'].fillna(all_preds_37['x'], inplace=True)\n",
    "all_preds_merge['y'].fillna(all_preds_37['y'], inplace=True)\n",
    "all_preds_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldの結果を平均した後、reindexでsubmission fileにindexを合わせる\n",
    "all_preds_merge.index = sub.index\n",
    "all_preds_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floorの数値を置換\n",
    "simple_accurate_99 = pd.read_csv('../01/submission.csv')\n",
    "all_preds_merge['floor'] = simple_accurate_99['floor'].values\n",
    "all_preds_merge.to_csv(str(OUTPUT_DIR) + f\"/sub{EXP_NAME}.csv\")\n",
    "all_preds_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df = pd.read_csv(str(OUTPUT_DIR) + f\"/oof{EXP_NAME}.csv\")\n",
    "sub_df = pd.read_csv(str(OUTPUT_DIR) + f\"/sub{EXP_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rel_positions(acce_datas, ahrs_datas):\n",
    "    step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n",
    "    headings = compute_f.compute_headings(ahrs_datas)\n",
    "    stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n",
    "    step_headings = compute_f.compute_step_heading(step_timestamps, headings)\n",
    "    rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n",
    "    return rel_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path(args):\n",
    "\n",
    "    path, path_df = args\n",
    "    T_ref  = path_df['timestamp'].values\n",
    "    xy_hat = path_df[['x', 'y']].values\n",
    "    txt_path = path_df['txt_path'].values[0]\n",
    "    \n",
    "    example = read_data_file(txt_path)\n",
    "    rel_positions = compute_rel_positions(example.acce, example.ahrs)\n",
    "    if T_ref[-1] > rel_positions[-1, 0]:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n",
    "    else:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n",
    "    rel_positions = np.concatenate(rel_positions)\n",
    "    \n",
    "    T_rel = rel_positions[:, 0]\n",
    "    delta_xy_hat = np.diff(scipy.interpolate.interp1d(T_rel, np.cumsum(rel_positions[:, 1:3], axis=0), axis=0)(T_ref), axis=0)\n",
    "\n",
    "    N = xy_hat.shape[0]\n",
    "    delta_t = np.diff(T_ref)\n",
    "    alpha = (8.1)**(-2) * np.ones(N)\n",
    "    beta  = (0.3 + 0.3 * 1e-3 * delta_t)**(-2)\n",
    "    A = scipy.sparse.spdiags(alpha, [0], N, N)\n",
    "    B = scipy.sparse.spdiags( beta, [0], N-1, N-1)\n",
    "    D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n",
    "\n",
    "    Q = A + (D.T @ B @ D)\n",
    "    c = (A @ xy_hat) + (D.T @ (B @ delta_xy_hat))\n",
    "    xy_star = scipy.sparse.linalg.spsolve(Q, c)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'site_path_timestamp' : path_df['site_path_timestamp'],\n",
    "        'floor' : path_df['floor'],\n",
    "        'x' : xy_star[:, 0],\n",
    "        'y' : xy_star[:, 1],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sub_df['site_path_timestamp'].apply(lambda s : pd.Series(s.split('_')))\n",
    "sub_df['site'] = tmp[0]\n",
    "sub_df['path'] = tmp[1]\n",
    "sub_df['timestamp'] = tmp[2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_buildings = sorted(sub_df['site'].value_counts().index.tolist())\n",
    "test_txts = sorted(glob.glob(str(DATA_DIR/'indoor-location-navigation') + f'/test/*.txt'))\n",
    "train_txts = [sorted(glob.glob(str(DATA_DIR/'indoor-location-navigation') + f'/train/{used_building}/*/*.txt')) for used_building in used_buildings]\n",
    "train_txts = sum(train_txts, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pathes = []\n",
    "for path in tqdm(sub_df['path'].values):\n",
    "    txt_pathes.append([test_txt for test_txt in test_txts if path in test_txt][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['txt_path'] = txt_pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(correct_path, sub_df.groupby('path'))\n",
    "    dfs = tqdm(dfs)\n",
    "    dfs = list(dfs)\n",
    "sub_df_cm = pd.concat(dfs).sort_values('site_path_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_pathes = []\n",
    "for path in tqdm(oofs_df['path'].values):\n",
    "    txt_pathes.append([train_txt for train_txt in train_txts if path in train_txt][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df['txt_path'] = txt_pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = multiprocessing.cpu_count()\n",
    "with multiprocessing.Pool(processes=processes) as pool:\n",
    "    dfs = pool.imap_unordered(correct_path, oofs_df.groupby('path'))\n",
    "    dfs = tqdm(dfs)\n",
    "    dfs = list(dfs)\n",
    "oofs_df_cm = pd.concat(dfs).sort_index()\n",
    "oofs_df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df_cm.to_csv(str(OUTPUT_DIR) + f\"/oof{EXP_NAME}_cm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_score = mean_position_error(\n",
    "        oofs_df_cm['x'], oofs_df_cm['y'], 0,\n",
    "        train_df['x'].values, train_df['y'].values, 0)\n",
    "print(f\"CV:{oofs_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_cm.to_csv(str(OUTPUT_DIR) + f\"/sub{EXP_NAME}_cm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df_cm = pd.read_csv(str(OUTPUT_DIR) + f\"/oof{EXP_NAME}_cm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_cm = pd.read_csv(str(OUTPUT_DIR) + f\"/sub{EXP_NAME}_cm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_col(df):\n",
    "    df = pd.concat([\n",
    "        df['site_path_timestamp'].str.split('_', expand=True) \\\n",
    "        .rename(columns={0:'site',\n",
    "                         1:'path',\n",
    "                         2:'timestamp'}),\n",
    "        df\n",
    "    ], axis=1).copy()\n",
    "    return df\n",
    "def sub_process(sub, train_waypoints):\n",
    "    train_waypoints['isTrainWaypoint'] = True\n",
    "    sub = split_col(sub[['site_path_timestamp','floor','x','y']]).copy()\n",
    "    sub = sub.merge(train_waypoints[['site','floorNo','floor']].drop_duplicates(), how='left')\n",
    "    sub = sub.merge(\n",
    "        train_waypoints[['x','y','site','floor','isTrainWaypoint']].drop_duplicates(),\n",
    "        how='left',\n",
    "        on=['site','x','y','floor']\n",
    "             )\n",
    "    sub['isTrainWaypoint'] = sub['isTrainWaypoint'].fillna(False)\n",
    "    return sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_waypoints = pd.read_csv(str(DATA_DIR/'indoor-location-navigation') + '/train_waypoints.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_cm = sub_process(sub_df_cm, train_waypoints)\n",
    "oofs_df_cm = sub_process(oofs_df_cm, train_waypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def add_xy(df):\n",
    "    df['xy'] = [(x, y) for x,y in zip(df['x'], df['y'])]\n",
    "    return df\n",
    "\n",
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    return points[cdist([point], points).argmin()]\n",
    "\n",
    "sub_df_cm = add_xy(sub_df_cm)\n",
    "train_waypoints = add_xy(train_waypoints)\n",
    "\n",
    "ds = []\n",
    "for (site, myfloor), d in tqdm(sub_df_cm.groupby(['site','floor'])):\n",
    "    true_floor_locs = train_waypoints.loc[(train_waypoints['floor'] == myfloor) &\n",
    "                                          (train_waypoints['site'] == site)] \\\n",
    "        .reset_index(drop=True)\n",
    "    if len(true_floor_locs) == 0:\n",
    "        print(f'Skipping {site} {myfloor}')\n",
    "        continue\n",
    "    d['matched_point'] = [closest_point(x, list(true_floor_locs['xy'])) for x in d['xy']]\n",
    "    d['x_'] = d['matched_point'].apply(lambda x: x[0])\n",
    "    d['y_'] = d['matched_point'].apply(lambda x: x[1])\n",
    "    ds.append(d)\n",
    "\n",
    "sub_df_cm_ds = pd.concat(ds)\n",
    "\n",
    "\n",
    "oofs_df_cm = add_xy(oofs_df_cm)\n",
    "train_waypoints = add_xy(train_waypoints)\n",
    "\n",
    "ds = []\n",
    "for (site, myfloor), d in tqdm(oofs_df_cm.groupby(['site','floor'])):\n",
    "    true_floor_locs = train_waypoints.loc[(train_waypoints['floor'] == myfloor) &\n",
    "                                          (train_waypoints['site'] == site)] \\\n",
    "        .reset_index(drop=True)\n",
    "    if len(true_floor_locs) == 0:\n",
    "        print(f'Skipping {site} {myfloor}')\n",
    "        continue\n",
    "    d['matched_point'] = [closest_point(x, list(true_floor_locs['xy'])) for x in d['xy']]\n",
    "    d['x_'] = d['matched_point'].apply(lambda x: x[0])\n",
    "    d['y_'] = d['matched_point'].apply(lambda x: x[1])\n",
    "    ds.append(d)\n",
    "\n",
    "oofs_df_cm_ds = pd.concat(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_to_grid(sub, threshold):\n",
    "    \"\"\"\n",
    "    Snap to grid if within a threshold.\n",
    "    \n",
    "    x, y are the predicted points.\n",
    "    x_, y_ are the closest grid points.\n",
    "    _x_, _y_ are the new predictions after post processing.\n",
    "    \"\"\"\n",
    "    sub['_x_'] = sub['x']\n",
    "    sub['_y_'] = sub['y']\n",
    "    sub.loc[sub['dist'] < threshold, '_x_'] = sub.loc[sub['dist'] < threshold]['x_']\n",
    "    sub.loc[sub['dist'] < threshold, '_y_'] = sub.loc[sub['dist'] < threshold]['y_']\n",
    "    return sub.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distances\n",
    "sub_df_cm_ds['dist'] = np.sqrt( (sub_df_cm_ds.x-sub_df_cm_ds.x_)**2 + (sub_df_cm_ds.y-sub_df_cm_ds.y_)**2 )\n",
    "sub_pp = snap_to_grid(sub_df_cm_ds, threshold=5)\n",
    "sub_pp = sub_pp[['site_path_timestamp','floor','_x_','_y_','site','path','floorNo']] \\\n",
    "    .rename(columns={'_x_':'x', '_y_':'y'})\n",
    "\n",
    "# Calculate the distances\n",
    "oofs_df_cm_ds['dist'] = np.sqrt( (oofs_df_cm_ds.x-oofs_df_cm_ds.x_)**2 + (oofs_df_cm_ds.y-oofs_df_cm_ds.y_)**2 )\n",
    "oofs_pp = snap_to_grid(oofs_df_cm_ds, threshold=5)\n",
    "oofs_pp = oofs_pp[['site_path_timestamp','floor','_x_','_y_','site','path','floorNo']] \\\n",
    "    .rename(columns={'_x_':'x', '_y_':'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pp = sub_pp.sort_index()\n",
    "sub_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pp[['site_path_timestamp','floor','x','y']] \\\n",
    "    .to_csv(str(OUTPUT_DIR) + f\"/sub{EXP_NAME}_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_pp = oofs_pp.sort_index()\n",
    "oofs_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_score = mean_position_error(\n",
    "        oofs_pp['x'], oofs_pp['y'], 0,\n",
    "        train_df['x'].values, train_df['y'].values, 0)\n",
    "print(f\"CV:{oofs_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_pp[['site_path_timestamp','floor','x','y']] \\\n",
    "    .to_csv(str(OUTPUT_DIR) + f\"/oof{EXP_NAME}_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='Indoor_Location_Navigation', entity='sqrt4kaido', group=RUN_NAME, job_type='summary')\n",
    "wandb.run.name = 'summary'\n",
    "wandb.log({'CV_score': oofs_score})\n",
    "wandb.save(utils.get_notebook_path())\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def plot_preds(\n",
    "    site,\n",
    "    floorNo,\n",
    "    sub=None,\n",
    "    true_locs=None,\n",
    "    base=str(DATA_DIR/'indoor-location-navigation'),\n",
    "    show_train=True,\n",
    "    show_preds=True,\n",
    "    fix_labels=True,\n",
    "    map_floor=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots predictions on floorplan map.\n",
    "    \n",
    "    map_floor : use a different floor's map\n",
    "    \"\"\"\n",
    "    if map_floor is None:\n",
    "        map_floor = floorNo\n",
    "    # Prepare width_meter & height_meter (taken from the .json file)\n",
    "    floor_plan_filename = f\"{base}/metadata/{site}/{map_floor}/floor_image.png\"\n",
    "    json_plan_filename = f\"{base}/metadata/{site}/{map_floor}/floor_info.json\"\n",
    "    with open(json_plan_filename) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    width_meter = json_data[\"map_info\"][\"width\"]\n",
    "    height_meter = json_data[\"map_info\"][\"height\"]\n",
    "\n",
    "    floor_img = plt.imread(f\"{base}/metadata/{site}/{map_floor}/floor_image.png\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    plt.imshow(floor_img)\n",
    "\n",
    "    if show_train:\n",
    "        true_locs = true_locs.query('site == @site and floorNo == @map_floor').copy()\n",
    "        true_locs[\"x_\"] = true_locs[\"x\"] * floor_img.shape[0] / height_meter\n",
    "        true_locs[\"y_\"] = (\n",
    "            true_locs[\"y\"] * -1 * floor_img.shape[1] / width_meter\n",
    "        ) + floor_img.shape[0]\n",
    "        true_locs.query(\"site == @site and floorNo == @map_floor\").groupby(\"path\").plot(\n",
    "            x=\"x_\",\n",
    "            y=\"y_\",\n",
    "            style=\"+\",\n",
    "            ax=ax,\n",
    "            label=\"train waypoint location\",\n",
    "            color=\"grey\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    if show_preds:\n",
    "        sub = sub.query('site == @site and floorNo == @floorNo').copy()\n",
    "        sub[\"x_\"] = sub[\"x\"] * floor_img.shape[0] / height_meter\n",
    "        sub[\"y_\"] = (\n",
    "            sub[\"y\"] * -1 * floor_img.shape[1] / width_meter\n",
    "        ) + floor_img.shape[0]\n",
    "        for path, path_data in sub.query(\n",
    "            \"site == @site and floorNo == @floorNo\"\n",
    "        ).groupby(\"path\"):\n",
    "            path_data.plot(\n",
    "                x=\"x_\",\n",
    "                y=\"y_\",\n",
    "                style=\".-\",\n",
    "                ax=ax,\n",
    "                title=f\"{site} - floor - {floorNo}\",\n",
    "                alpha=1,\n",
    "                label=path,\n",
    "            )\n",
    "    if fix_labels:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        plt.legend(\n",
    "            by_label.values(), by_label.keys(), loc=\"center left\", bbox_to_anchor=(1, 0.5)\n",
    "        )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site = '5a0546857ecc773753327266'\n",
    "example_floorNo = 'F1'\n",
    "\n",
    "sub_df = sub_process(sub_df, train_waypoints)\n",
    "plot_preds(example_site, example_floorNo, sub_df,\n",
    "           train_waypoints, show_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(example_site, example_floorNo, sub_df_cm,\n",
    "           train_waypoints, show_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_preds(example_site, example_floorNo, sub_pp,\n",
    "           train_waypoints, show_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
